pip install streamlit openai llama-index llama-index-experimental


import streamlit as st
import openai
from llama_index.core import VectorStoreIndex
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.prompts import ChatPromptTemplate
from llama_index.core.base.llms.types import ChatMessage, MessageRole

# Streamlit UI Setup
st.set_page_config(page_title="GPT Chatbot", layout="wide")
st.title("GPT-3.5 Turbo Chatbot with Custom API Key")

# User inputs OpenAI API Key
api_key = st.text_input("Enter your OpenAI API Key:", type="password")
if not api_key:
    st.warning("Please enter your OpenAI API Key to continue.")
    st.stop()

# Set API key for OpenAI
openai.api_key = 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

# Define meta-prompt
CUSTOM_PROMPT = [
    ChatMessage(
        content=(
            "You are a conversational assistant designed to encourage exploration and discovery through thoughtful questions."
            "Start interactions by asking open-ended questions relevant to the user's response, with the aim of understanding their perspective and goals better."
            "Only provide instructions or guidance when explicitly requested or when it becomes clear that offering help will improve the conversation."
            "Always answer the query using only the provided context information."
            "Some rules to follow:"
            "1. Never directly reference the given context in your answer."
            "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines."
        ),
        role=MessageRole.SYSTEM,
    ),
    ChatMessage(
        content=(
            "Context information from multiple sources is below."
            "---------------------"
            "{context_str}"
            "---------------------"
            "Given the information from multiple sources and not prior knowledge, answer the query."
            "Query: {query_str}"
            "Answer: "
        ),
        role=MessageRole.USER,
    ),
]

CHAT_PROMPT = ChatPromptTemplate(message_templates=CUSTOM_PROMPT)

# Initialize Chat Memory
memory = ChatMemoryBuffer.from_defaults(token_limit=80000)

# Create Chat Engine
vectorstoreindex = VectorStoreIndex([])  # Empty vector store as placeholder
chat_engine = vectorstoreindex.as_chat_engine(
    chat_mode="context",
    memory=memory,
    system_prompt=(
        "Context information from multiple sources is below."
        "---------------------"
        "{context_str}"
        "---------------------"
        "Given the information from multiple sources, answer the query."
        "If the query is unrelated to the context, just answer: I don't know."
        "Query: {query_str}"
        "Answer: "
    ),
)

chat_engine.reset()

# Chat interface
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

user_input = st.chat_input("Ask a question...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    response = chat_engine.chat(user_input)
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
